{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization Lab\n",
    "\n",
    "In this lab, you will be leveraging several concepts you have learned to obtain a list of links from a web page and crawl and index the pages referenced by those links - both sequentially and in parallel. Follow the steps below to complete the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Use the requests library to retrieve the content from the URL below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Data_science'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(url)\n",
    "r.status_code\n",
    "#r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Use BeautifulSoup to extract a list of all the unique links on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas las ligas encontradas en las etiquetas a: 470\n",
      "Ligas despùes de eliminar duplicados 414\n"
     ]
    }
   ],
   "source": [
    "#Extraer las etiquetas <a> con href\n",
    "sopa = bs(r.text,'lxml')\n",
    "links=[a for a in sopa.find_all('a', href=True)]\n",
    "print('Todas las ligas encontradas en las etiquetas a:' ,len(links))\n",
    "links = set(links)\n",
    "links = list(links)\n",
    "print('Ligas despùes de eliminar duplicados', len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use list comprehensions with conditions to clean the link list.\n",
    "\n",
    "There are two types of links, absolute and relative. Absolute links have the full URL and begin with http while relative links begin with a forward slash (/) and point to an internal page within the wikipedia.org domain. Clean the respective types of URLs as follows.\n",
    "\n",
    "- Absolute Links: Create a list of these and remove any that contain a percentage sign (%).\n",
    "- Relativel Links: Create a list of these, add the domain to the link so that you have the full URL, and remove any that contain a percentage sign (%).\n",
    "- Combine the list of absolute and relative links and ensure there are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'http://wikipedia.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links extraidos de la sopa: 69\n"
     ]
    }
   ],
   "source": [
    "#Absolute Links  - Extraer las etiquetas \"a\" con class \"external text\"\n",
    "sopa = bs(r.text,'lxml')\n",
    "links_abs=[a for a in sopa.find_all('a', {'class':'external text'})]\n",
    "print('Links extraidos de la sopa:',len(links_abs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totalidad de Links absolutos: 69\n",
      "Totalidad de Links absolutos sin %: 60\n",
      "Links absolutos únicos: 58\n"
     ]
    }
   ],
   "source": [
    "#Quitar etiquetas y href y title con su texto\n",
    "for i in range(len(links_abs)):\n",
    "    links_abs[i] = str(links_abs[i])\n",
    "    links_abs[i] = links_abs[i].replace('<a class=\"external text\" href=\"','')\n",
    "    fin = links_abs[i].find('\"')\n",
    "    links_abs[i] = links_abs[i][:fin]\n",
    "print('Totalidad de Links absolutos:',len(links_abs))\n",
    "#Quirar las que tinen %\n",
    "for i in range(len(links_abs)):\n",
    "    if links_abs[i].find('%') >= 0:\n",
    "        links_abs[i] = False\n",
    "\n",
    "for i in range(links_abs.count(False)):\n",
    "    links_abs.remove(False)\n",
    "\n",
    "print('Totalidad de Links absolutos sin %:',len(links_abs))\n",
    "\n",
    "links_abs = set(links_abs)\n",
    "links_abs = list(links_abs)\n",
    "print('Links absolutos únicos:',len(links_abs))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links extraidos de la sopa: 470\n"
     ]
    }
   ],
   "source": [
    "#Relative - Links: Marcar como false todo lo que no empieze con <a href=\n",
    "sopa = bs(r.text,'lxml')\n",
    "links=[a for a in sopa.find_all('a', href=True)]\n",
    "print('Links extraidos de la sopa:',len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despues de quitar los que no son ligas relativas y que tienen % quedan: 205\n"
     ]
    }
   ],
   "source": [
    "#Marcar los que no empiezan con '<a href=\"/wiki/' y que tienen %\n",
    "\n",
    "for i in range(len(links)):\n",
    "    links[i]=str(links[i])\n",
    "    if links[i].find('<a href=\"/wiki/') !=0 or links[i].find('%') >= 0:\n",
    "        links[i]=False\n",
    "\n",
    "#Remover de la lista los False\n",
    "for i in range(links.count(False)):\n",
    "    links.remove(False)\n",
    "print('Despues de quitar los que no son ligas relativas y que tienen % quedan:',len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitar etiquetas y href y title con su texto\n",
    "for i in range(len(links)):\n",
    "    links[i]=links[i].replace('<a href=\"',domain)\n",
    "    fin=links[i].find('\"')\n",
    "    links[i]=links[i][:fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links relativos extraidos: 205\n",
      "Links relativos unicos: 157\n"
     ]
    }
   ],
   "source": [
    "print('Links relativos extraidos:',len(links))\n",
    "links = set(links)\n",
    "links = list(links)\n",
    "print('Links relativos unicos:',len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de links: 215\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(links_abs)):\n",
    "    links.append(links_abs[i]) \n",
    "print('Total de links:',len(links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://wikipedia.org/wiki/Open_science',\n",
       " 'http://wikipedia.org/wiki/Relevance_vector_machine',\n",
       " 'http://wikipedia.org/wiki/Artificial_neural_network',\n",
       " 'http://wikipedia.org/wiki/Temporal_difference_learning',\n",
       " 'http://wikipedia.org/wiki/Machine_Learning_(journal)',\n",
       " 'http://wikipedia.org/wiki/Distributed_computing',\n",
       " 'http://wikipedia.org/wiki/Statistical_learning_theory',\n",
       " 'http://wikipedia.org/wiki/Jeff_Hammerbacher',\n",
       " 'http://wikipedia.org/wiki/Graphical_model',\n",
       " 'http://wikipedia.org/wiki/Pattern_recognition',\n",
       " 'http://wikipedia.org/wiki/New_York_University',\n",
       " 'http://wikipedia.org/wiki/Information_explosion',\n",
       " 'http://wikipedia.org/wiki/DBSCAN',\n",
       " 'http://wikipedia.org/wiki/Predictive_modelling',\n",
       " 'http://wikipedia.org/wiki/Social_science',\n",
       " 'http://wikipedia.org/wiki/Computational_learning_theory',\n",
       " 'http://wikipedia.org/wiki/Ensemble_learning',\n",
       " 'http://wikipedia.org/wiki/Hierarchical_clustering',\n",
       " 'http://wikipedia.org/wiki/David_Donoho',\n",
       " 'http://wikipedia.org/wiki/Feature_engineering',\n",
       " 'http://wikipedia.org/wiki/Analytics',\n",
       " 'http://wikipedia.org/wiki/Basic_research',\n",
       " 'http://wikipedia.org/wiki/Academy',\n",
       " 'http://wikipedia.org/wiki/University_of_Michigan',\n",
       " 'http://wikipedia.org/wiki/Outline_of_machine_learning',\n",
       " 'http://wikipedia.org/wiki/Special:BookSources/0-12-241770-4',\n",
       " 'http://wikipedia.org/wiki/Category:Use_dmy_dates_from_December_2012',\n",
       " 'http://wikipedia.org/wiki/Wikipedia:Featured_content',\n",
       " 'http://wikipedia.org/wiki/Nate_Silver',\n",
       " 'http://wikipedia.org/wiki/International_Conference_on_Machine_Learning',\n",
       " 'http://wikipedia.org/wiki/K-nearest_neighbors_algorithm',\n",
       " 'http://wikipedia.org/wiki/Support-vector_machine',\n",
       " 'http://wikipedia.org/wiki/K-means_clustering',\n",
       " 'http://wikipedia.org/wiki/Long_short-term_memory',\n",
       " 'http://wikipedia.org/wiki/Interdisciplinarity',\n",
       " 'http://wikipedia.org/wiki/Empirical_research',\n",
       " 'http://wikipedia.org/wiki/Bootstrap_aggregating',\n",
       " 'http://wikipedia.org/wiki/List_of_datasets_for_machine-learning_research',\n",
       " 'http://wikipedia.org/wiki/Independent_component_analysis',\n",
       " 'http://wikipedia.org/wiki/Indian_Statistical_Institute',\n",
       " 'http://wikipedia.org/wiki/Gated_recurrent_unit',\n",
       " 'http://wikipedia.org/wiki/Discipline_(academia)',\n",
       " 'http://wikipedia.org/wiki/Linear_discriminant_analysis',\n",
       " 'http://wikipedia.org/wiki/Computer_science',\n",
       " 'http://wikipedia.org/wiki/BIRCH',\n",
       " 'http://wikipedia.org/wiki/Perceptron',\n",
       " 'http://wikipedia.org/wiki/Unstructured_data',\n",
       " 'http://wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_April_2018',\n",
       " 'http://wikipedia.org/wiki/U-Net',\n",
       " 'http://wikipedia.org/wiki/General_Assembly_(school)',\n",
       " 'http://wikipedia.org/wiki/Paradigm',\n",
       " 'http://wikipedia.org/wiki/Help:Contents',\n",
       " 'http://wikipedia.org/wiki/Q-learning',\n",
       " 'http://wikipedia.org/wiki/Dimensionality_reduction',\n",
       " 'http://wikipedia.org/wiki/Jim_Gray_(computer_scientist)',\n",
       " 'http://wikipedia.org/wiki/Supervised_learning',\n",
       " 'http://wikipedia.org/wiki/International_Standard_Book_Number',\n",
       " 'http://wikipedia.org/wiki/Regression_analysis',\n",
       " 'http://wikipedia.org/wiki/Random_forest',\n",
       " 'http://wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems',\n",
       " 'http://wikipedia.org/wiki/Information_science',\n",
       " 'http://wikipedia.org/wiki/Unsupervised_learning',\n",
       " 'http://wikipedia.org/wiki/Harvard_Business_Review',\n",
       " 'http://wikipedia.org/wiki/The_Wall_Street_Journal',\n",
       " 'http://wikipedia.org/wiki/Special:BookSources/9784431702085',\n",
       " 'http://wikipedia.org/wiki/Probably_approximately_correct_learning',\n",
       " 'http://wikipedia.org/wiki/Statistics',\n",
       " 'http://wikipedia.org/wiki/Data_analysis',\n",
       " 'http://wikipedia.org/wiki/DJ_Patil',\n",
       " 'http://wikipedia.org/wiki/Data_set',\n",
       " 'http://wikipedia.org/wiki/Statistical_theory',\n",
       " 'http://wikipedia.org/wiki/Wikipedia:General_disclaimer',\n",
       " 'http://wikipedia.org/wiki/Digital_object_identifier',\n",
       " 'http://wikipedia.org/wiki/Knowledge',\n",
       " 'http://wikipedia.org/wiki/Structured_prediction',\n",
       " 'http://wikipedia.org/wiki/Thomas_H._Davenport',\n",
       " 'http://wikipedia.org/wiki/Wikipedia:Citation_needed',\n",
       " 'http://wikipedia.org/wiki/Anomaly_detection',\n",
       " 'http://wikipedia.org/wiki/Linear_regression',\n",
       " 'http://wikipedia.org/wiki/Template_talk:Machine_learning_bar',\n",
       " 'http://wikipedia.org/wiki/Restricted_Boltzmann_machine',\n",
       " 'http://wikipedia.org/wiki/Category:Data_analysis',\n",
       " 'http://wikipedia.org/wiki/Mathematics',\n",
       " 'http://wikipedia.org/wiki/Category:Computer_occupations',\n",
       " 'http://wikipedia.org/wiki/Association_rule_learning',\n",
       " 'http://wikipedia.org/wiki/Conditional_random_field',\n",
       " 'http://wikipedia.org/wiki/Learning_to_rank',\n",
       " 'http://wikipedia.org/wiki/Statistical_classification',\n",
       " 'http://wikipedia.org/wiki/Reinforcement_learning',\n",
       " 'http://wikipedia.org/wiki/Methodology',\n",
       " 'http://wikipedia.org/wiki/Local_outlier_factor',\n",
       " 'http://wikipedia.org/wiki/Machine_learning',\n",
       " 'http://wikipedia.org/wiki/Big_data',\n",
       " 'http://wikipedia.org/wiki/Non-negative_matrix_factorization',\n",
       " 'http://wikipedia.org/wiki/Peter_Naur',\n",
       " 'http://wikipedia.org/wiki/Hidden_Markov_model',\n",
       " 'http://wikipedia.org/wiki/International_Standard_Serial_Number',\n",
       " 'http://wikipedia.org/wiki/Deep_learning',\n",
       " 'http://wikipedia.org/wiki/Principal_component_analysis',\n",
       " 'http://wikipedia.org/wiki/Cluster_analysis',\n",
       " 'http://wikipedia.org/wiki/Glossary_of_artificial_intelligence',\n",
       " 'http://wikipedia.org/wiki/Occam_learning',\n",
       " 'http://wikipedia.org/wiki/Wikipedia:Contents',\n",
       " 'http://wikipedia.org/wiki/Category:Computational_fields_of_study',\n",
       " 'http://wikipedia.org/wiki/Boosting_(machine_learning)',\n",
       " 'http://wikipedia.org/wiki/Naive_Bayes_classifier',\n",
       " 'http://wikipedia.org/wiki/Graduate_school',\n",
       " 'http://wikipedia.org/wiki/Statistician',\n",
       " 'http://wikipedia.org/wiki/Portal:Current_events',\n",
       " 'http://wikipedia.org/wiki/Generative_adversarial_network',\n",
       " 'http://wikipedia.org/wiki/Bayesian_network',\n",
       " 'http://wikipedia.org/wiki/Template:Machine_learning_bar',\n",
       " 'http://wikipedia.org/wiki/Wikipedia:Community_portal',\n",
       " 'http://wikipedia.org/wiki/Online_machine_learning',\n",
       " 'http://wikipedia.org/wiki/Category:All_articles_with_unsourced_statements',\n",
       " 'http://wikipedia.org/wiki/Multilayer_perceptron',\n",
       " 'http://wikipedia.org/wiki/Self-organizing_map',\n",
       " 'http://wikipedia.org/wiki/American_Statistical_Association',\n",
       " 'http://wikipedia.org/wiki/Category:Information_science',\n",
       " 'http://wikipedia.org/wiki/PubMed_Central',\n",
       " 'http://wikipedia.org/wiki/Data_mining',\n",
       " 'http://wikipedia.org/wiki/Data_science',\n",
       " 'http://wikipedia.org/wiki/National_Institutes_of_Health',\n",
       " 'http://wikipedia.org/wiki/Applied_science',\n",
       " 'http://wikipedia.org/wiki/Jeffrey_T._Leek',\n",
       " 'http://wikipedia.org/wiki/Factor_analysis',\n",
       " 'http://wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding',\n",
       " 'http://wikipedia.org/wiki/OPTICS_algorithm',\n",
       " 'http://wikipedia.org/wiki/Help:Category',\n",
       " 'http://wikipedia.org/wiki/Wayback_Machine',\n",
       " 'http://wikipedia.org/wiki/Industry',\n",
       " 'http://wikipedia.org/wiki/Grammar_induction',\n",
       " 'http://wikipedia.org/wiki/Decision_tree_learning',\n",
       " 'http://wikipedia.org/wiki/Wikipedia:About',\n",
       " 'http://wikipedia.org/wiki/Computational_science',\n",
       " 'http://wikipedia.org/wiki/DeepDream',\n",
       " 'http://wikipedia.org/wiki/The_Data_Incubator',\n",
       " 'http://wikipedia.org/wiki/University_of_Essex',\n",
       " 'http://wikipedia.org/wiki/NYU_Stern_Center_for_Business_and_Human_Rights',\n",
       " 'http://wikipedia.org/wiki/Prasanta_Chandra_Mahalanobis',\n",
       " 'http://wikipedia.org/wiki/Empirical_risk_minimization',\n",
       " 'http://wikipedia.org/wiki/Category:Webarchive_template_wayback_links',\n",
       " 'http://wikipedia.org/wiki/Theory',\n",
       " 'http://wikipedia.org/wiki/Logistic_regression',\n",
       " 'http://wikipedia.org/wiki/Feature_learning',\n",
       " 'http://wikipedia.org/wiki/Journal_of_Machine_Learning_Research',\n",
       " 'http://wikipedia.org/wiki/Convolutional_neural_network',\n",
       " 'http://wikipedia.org/wiki/Automated_machine_learning',\n",
       " 'http://wikipedia.org/wiki/Academic_journal',\n",
       " 'http://wikipedia.org/wiki/Computing',\n",
       " 'http://wikipedia.org/wiki/Recurrent_neural_network',\n",
       " 'http://wikipedia.org/wiki/Database',\n",
       " 'http://wikipedia.org/wiki/Special:BookSources/978-0-9825442-0-4',\n",
       " 'http://wikipedia.org/wiki/Academic_publishing',\n",
       " 'http://wikipedia.org/wiki/Montpellier_2_University',\n",
       " 'http://wikipedia.org/wiki/Semi-supervised_learning',\n",
       " 'http://wikipedia.org/wiki/Autoencoder',\n",
       " 'https://web.archive.org/web/20180323043716/http://dsaa.co/',\n",
       " 'https://web.archive.org/web/20141109113411/http://cacm.acm.org/magazines/2013/12/169933-data-science-and-prediction/fulltext',\n",
       " '//pubmed.ncbi.nlm.nih.gov/24482835',\n",
       " 'https://web.archive.org/web/20170329172857/http://datamining.it.uts.edu.au/conferences/dsaa14/',\n",
       " 'https://www.springer.com/book/9784431702085',\n",
       " 'https://web.archive.org/web/20130703121219/http://www.nsf.gov/pubs/2005/nsb0540/',\n",
       " 'https://web.archive.org/web/20180619003326/http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf',\n",
       " 'https://web.archive.org/web/20180426153402/https://www.nytimes.com/2009/12/15/science/15books.html',\n",
       " '//www.worldcat.org/issn/0362-4331',\n",
       " 'https://web.archive.org/web/20170205101231/http://magazine.amstat.org/blog/2016/06/01/datascience-2/',\n",
       " 'http://simplystatistics.org/2013/12/12/the-key-word-in-data-science-is-not-data-it-is-science/',\n",
       " 'http://www.statisticsviews.com/details/feature/5133141/Nate-Silver-What-I-need-from-statisticians.html',\n",
       " 'https://web.archive.org/web/20141014022300/http://www2.isye.gatech.edu/~jeffwu/presentations/datascience.pdf',\n",
       " '//www.worldcat.org/issn/1465-4644',\n",
       " 'https://web.archive.org/web/20171124155559/https://blogs.wsj.com/cio/2014/05/02/why-do-we-need-data-science-when-weve-had-statistics-for-centuries/',\n",
       " '//www.worldcat.org/issn/0028-0836',\n",
       " 'https://web.archive.org/web/20131029191813/http://www.isical.ac.in/~statmath/html/pcm/pcm_recent.html',\n",
       " 'https://www.springer.com/41060',\n",
       " 'http://www2.isye.gatech.edu/~jeffwu/presentations/datascience.pdf',\n",
       " '//www.worldcat.org/issn/0036-8075',\n",
       " 'http://www.isical.ac.in/~statmath/html/pcm/pcm_recent.html',\n",
       " 'https://web.archive.org/web/20120822033955/http://www.jds-online.com/v1-1',\n",
       " 'http://www.nsf.gov/pubs/2005/nsb0540/',\n",
       " 'https://blogs.wsj.com/cio/2014/05/02/why-do-we-need-data-science-when-weve-had-statistics-for-centuries/',\n",
       " 'http://www.gfkl.org/welcome/',\n",
       " 'http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf',\n",
       " 'http://www.dsaa.co',\n",
       " 'https://web.archive.org/web/20160215235820/http://venturebeat.com/2014/04/15/ny-gets-new-bootcamp-for-data-scientists-its-free-but-harder-to-get-into-than-harvard/',\n",
       " 'https://web.archive.org/web/20140102194117/http://simplystatistics.org/2013/12/12/the-key-word-in-data-science-is-not-data-it-is-science/',\n",
       " '//pubmed.ncbi.nlm.nih.gov/19535325',\n",
       " 'https://web.archive.org/web/20131029200232/http://ur.umich.edu/9899/Nov09_98/4.htm',\n",
       " 'https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/',\n",
       " 'https://www.nytimes.com/2009/12/15/science/15books.html',\n",
       " 'http://magazine.amstat.org/blog/2016/06/01/datascience-2/',\n",
       " 'https://web.archive.org/web/20120403153707/http://www.jstage.jst.go.jp/browse/dsj/_vols',\n",
       " 'https://web.archive.org/web/20170911223224/https://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/',\n",
       " 'https://web.archive.org/web/20090609022730/http://www.jstage.jst.go.jp/browse/dsj/1/0/_contents',\n",
       " 'https://web.archive.org/web/20170320193019/https://books.google.com/books?id=oGs_AQAAIAAJ',\n",
       " 'https://arxiv.org/list/cs.LG/recent',\n",
       " 'https://web.archive.org/web/20180215023354/https://pdfs.semanticscholar.org/915c/d8e2b39eb02723553913d592b2237d4d9960.pdf',\n",
       " 'http://cacm.acm.org/magazines/2013/12/169933-data-science-and-prediction/fulltext',\n",
       " 'https://web.archive.org/web/20190620184935/https://magazine.amstat.org/blog/2015/10/01/asa-statement-on-the-role-of-statistics-in-data-science/',\n",
       " 'http://datamining.it.uts.edu.au/conferences/dsaa14/',\n",
       " 'https://web.archive.org/web/20080511160948/http://www.codata.org/',\n",
       " 'https://books.google.com/books?id=oGs_AQAAIAAJ',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit',\n",
       " 'https://web.archive.org/web/20160202061421/https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/',\n",
       " 'https://venturebeat.com/2014/04/15/ny-gets-new-bootcamp-for-data-scientists-its-free-but-harder-to-get-into-than-harvard/',\n",
       " '//www.ncbi.nlm.nih.gov/pmc/articles/PMC4058759',\n",
       " 'https://magazine.amstat.org/blog/2015/10/01/asa-statement-on-the-role-of-statistics-in-data-science/',\n",
       " 'https://web.archive.org/web/20130823091106/http://www.statisticsviews.com/details/feature/5133141/Nate-Silver-What-I-need-from-statisticians.html',\n",
       " 'https://pdfs.semanticscholar.org/915c/d8e2b39eb02723553913d592b2237d4d9960.pdf',\n",
       " '//www.worldcat.org/issn/0960-3174',\n",
       " '//pubmed.ncbi.nlm.nih.gov/24436391',\n",
       " 'http://ur.umich.edu/9899/Nov09_98/4.htm',\n",
       " '//pubmed.ncbi.nlm.nih.gov/19265007',\n",
       " 'http://euads.org',\n",
       " 'https://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use the os library to create a folder called *wikipedia* and make that the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('./wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/al/Documents/ironhack/datamex0120/module-1/lab-parallelization/your-code'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('./wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/al/Documents/ironhack/datamex0120/module-1/lab-parallelization/your-code/wikipedia'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Write a function called index_page that accepts a link and does the following.\n",
    "\n",
    "- Tries to request the content of the page referenced by that link.\n",
    "- Slugifies the filename using the `slugify` function from the [python-slugify](https://pypi.org/project/python-slugify/) library and adds a .html file extension.\n",
    "    - If you don't already have the python-slugify library installed, you can pip install it as follows: `$ pip install python-slugify`.\n",
    "    - To import the slugify function, you would do the following: `from slugify import slugify`.\n",
    "    - You can then slugify a link as follows `slugify(link)`.\n",
    "- Creates a file in the wikipedia folder using the slugified filename and writes the contents of the page to the file.\n",
    "- If an exception occurs during the process above, just `pass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-slugify\n",
      "  Downloading https://files.pythonhosted.org/packages/92/5f/7b84a0bba8a0fdd50c046f8b57dcf179dc16237ad33446079b7c484de04c/python-slugify-4.0.0.tar.gz\n",
      "Collecting text-unidecode>=1.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/a5/c0b6468d3824fe3fde30dbb5e1f687b291608f9473681bbf7dabbf5a87d7/text_unidecode-1.3-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 936kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: python-slugify\n",
      "  Building wheel for python-slugify (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-slugify: filename=python_slugify-4.0.0-py2.py3-none-any.whl size=5487 sha256=dff0bc9748d6bec23b8b34a1cdcdb288026eb3f4e579ce0767108b8d3a09667d\n",
      "  Stored in directory: /Users/al/Library/Caches/pip/wheels/11/94/81/312969455540cb0e6a773e5d68a73c14128bfdfd4a7969bb4f\n",
      "Successfully built python-slugify\n",
      "Installing collected packages: text-unidecode, python-slugify\n",
      "Successfully installed python-slugify-4.0.0 text-unidecode-1.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install python-slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://wikipedia.org/wiki/Open_science\n",
      "http-wikipedia-org-wiki-open-science.html\n"
     ]
    }
   ],
   "source": [
    "print(links[0])\n",
    "print(slugify(links[0])+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('name.txt','w+')\n",
    "f.write('Esto es una linea')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def linkstofile(links,print_results=True):\n",
    "    contex=0\n",
    "    contok=0\n",
    "    contop=0\n",
    "    contre=0\n",
    "    contwr=0\n",
    "    contcl=0\n",
    "    #tqdm._instances.clear()\n",
    "    for i in tqdm(links):\n",
    "        try:\n",
    "            #crear archivo con slufied name\n",
    "            f = open(slugify(i)+'.html','w+')\n",
    "            contop+=1\n",
    "            #request\n",
    "            r = requests.get(i)\n",
    "            contre+=1\n",
    "            #escribir resultado del request en el archivo\n",
    "            f.write(r.text)\n",
    "            contwr=+1\n",
    "            f.close()\n",
    "            contok+=1\n",
    "        except:\n",
    "            contex+=1\n",
    "    \n",
    "    if print_results == True:\n",
    "        print('extraidas: {} \\nabortadas: {}\\nopen:{}\\nrequest\\nwrite{}:{}'\n",
    "              .format(contok,contex,contop,contre,contwr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Sequentially loop through the list of links, running the index_page function each time.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 123/215 [01:46<01:19,  1.16it/s]"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "linkstofile(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Perform the page indexing in parallel and note the difference in performance.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pool(linkstofile(links))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
